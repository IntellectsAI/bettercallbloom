{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cde69b99-b100-4dbd-999a-f02eef64290e",
   "metadata": {},
   "source": [
    "### Resources for finetuning:\n",
    "https://github.com/huggingface/notebooks/blob/main/examples/language_modeling.ipynb\n",
    " \n",
    "https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "https://github.com/huggingface/transformers/tree/main/notebooks\n",
    "\n",
    "Paper:\n",
    "https://arxiv.org/pdf/2205.01068.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "838bf6d2-9929-4cd0-87d9-61b76c5eda98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset pile_of_law (/Users/thomasrochefort/.cache/huggingface/datasets/pile-of-law___pile_of_law/r_legaladvice/0.0.0/acacf3e29a952ba9026148b979cb438151ebd33f842f5779a213967033c88619)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d37693857cc4cca8c17c1f6c77e8951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"pile-of-law/pile-of-law\",'r_legaladvice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f0f9d7-db8a-49f4-9085-3f289276c210",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>created_timestamp</th>\n",
       "      <th>downloaded_timestamp</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: I was told I can't come back to school due to medical condition [New York]\\nQuestion:[deleted]\\nAnswer #1: I want to add a few points: \\n\\nGrad school is different than undergraduate work. Fields move quickly, in some cases, and, regardless of outside issues, if you haven't reached the point of taking your exams by your fourth year the school may opt not to allow you to return. Keep in mind that being unable to complete coursework does not speak to your ability to complete dissertation work successfully.\\n\\nAssuming that you are funded, you are likely tying up a funding line for each year you are there, including when you take leave, and the school may not want to continue to fund you. \\n\\nAs to the crux of your complaint, schools have to make a reasonable accommodation for a medical issue or illness. Letting you take three significant gaps in a four year program is a lot to ask, and may indicate to them that you have attempted to return without being sufficiently prepared. The fact that you considered dropping out of the program may also affect how they see you as a student.Answer #2: INAL but was in a nearly identical situation to you (also had to leave school for diagnosed depression and anxiety disorders). From what I've seen, most schools have a limit on how long they let you leave for medical reasons. At the first school I went to, students were allowed two quarters of personal leave and eight quarters of medical leave. If you had to take time off beyond that, you were (and I was) dropped from the school. Depending on the circumstances, some schools will let you reapply to enter them again, but that's a very school and situation specific thing. \\n\\nI ended up transferring out of the first school to a community college while I dealt with the medical issue, and then into another four year university to finish my degree. If it comes down to it, you might have to do the same. Answer #3: Part of it could also be their accreditation agency stipulates that you must finish a degree program within a certain length of time. I know at the school I went to I had 1.5 times the length of the program to finish otherwise I would have to start over. It could simply be that you are getting dangerously close to that point and it has nothing to do with the medical condition.Answer #4: Public or private? Even with ADA protection, a private college can essentially drop you for no reasons. That does not mean you're not entitled to pursue legal ramifications, however it tends to be more difficult. \\n\\nDo you have any scholarships or grants? If so, what? The cost occurred via your leaves may have left a financial burden on the institution.\\n\\nDid you leaves cause any monetary repercussions? \\nSee above.\\n\\nDid you withdraw past the deadlines for add/drop? You illness may not require said documentation prior to the deadlines, but it may be applicable if suit follows.\\n\\nEssentially, the college's defense would be along he lines of whether or not they reasonably accommodated your disability, and their arguments would be accompanied by wages lost whilst doing so.</td>\n",
       "      <td>02-23-2016</td>\n",
       "      <td>11-10-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/476b71/i_was_told_i_cant_come_back_to_school_due_to/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: If you're furloughed and you dont want to go back to work, can you continue taking unemployment?\\nQuestion:My wife works in retail in Houston, and the store she works at is looking like it's opening up this Friday, they average over 1200 customer interactions a day on an avg weekend. She isn't comfortable going back to that with all the caronavirus stuff still going on, if she didnt return, could she continue to take unemployment until she finds a new career opportunity? Or would it be stopped if they said for her to come back?\\nAnswer #1: If they give her her job back, and she chooses not to go, now she's quit and no longer eligible for unemployment.Answer #2: If she's offered her job back and denies it, then she could (and most likely would) lose unemployment benefits.</td>\n",
       "      <td>04-27-2020</td>\n",
       "      <td>09-24-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/g9d92n/if_youre_furloughed_and_you_dont_want_to_go_back/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: (MD) Wife being represented by attorney whose retainer I paid in unrelated custody case\\nQuestion:Several years ago, my wife and I were battling my stepdaughter's father over custody concerns. I sold a number of my possessions and emptied my savings to pay the retainer for \"our\" lawyer (though I was not a party in the case), and interacted with him on at least one occasion. We are now on shaky ground and I have learned that she is seeking to have him represent her against me for divorce. Does this pose an ethical conflict for him?\\nAnswer #1: &amp;gt;Does this pose an ethical conflict for him?\\n\\nVery unlikely, unless you told him something at the time that could be used against you now.</td>\n",
       "      <td>10-23-2018</td>\n",
       "      <td>09-28-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/9qsast/md_wife_being_represented_by_attorney_whose/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: South Carolina- I was hired to work a job and was let go after working a few days and wasn't given a reason and haven't been paid yet, I'm 16\\nQuestion:I went to a job interview for a retail store that was opening in my town and along with the 3 other people that were at the interview I was hired on the spot. In total they hired about 40 people. They told us what day to come in for training and at training we filled out W4s and I had to show two forms of ID. Then for 4 days we all worked 8 hour shifts and then on the last day they told us the store only had 15 permanent positions and that not all of us were getting hired. I thought I already was hired. So basically I worked 32 hours and they never said I wasn't a permanent employee and they didn't give me a paycheck. This was on Monday and I still haven't been given one. Is it legal for them to not hire me even after I worked 32 hours and they never once told me it wasn't a permanent position, and if it's not legal who can I report them to? And how long do they have to give me a paycheck? I was supposed to get $9 an hour and I worked 32 hours so it should be $288 and I need that money. Does it matter that I'm a minor? Thank you\\nAnswer #1: They owe you the money but it's fine that they fired you. \\n\\nHave they told you that you won't be paid? Have you asked?</td>\n",
       "      <td>02-11-2017</td>\n",
       "      <td>11-08-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/5tj48i/south_carolina_i_was_hired_to_work_a_job_and_was/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title: [US-TX] renters rights. moved out three months before lease was up, landlord has been using time to remodel instead of find new tenants like he told us.\\nQuestion:i'll try to keep this short. bought and moved into new home in January 2015. lease is up on our rent house at the end of May 2015. landlord said he would immediatly start advertising the vaccancy but we found out he hasn't tried at all. he's been using the time to remodel the house. sucks for us as we've been struggling to make BOTH rent and house payment.\\n\\nthen i find this: \"Texas law (Tex. Prop. Code Ann. § 91.006), your landlord must make reasonable efforts to re-rent your unit—no matter what your reason for leaving—rather than charge you for the total remaining rent due under the lease.\"\\n\\ndoes our situation fall under this law? is there anything else i'm missing that could help us break our lease legally? any and all help would be greatly appreciated before i go talk to a attorney. thank you very much!\\nTopic:\\nLandlord Tenant Housing\\nAnswer #1: You can stop paying rent at anytime and force the landlord to sue you for the money. Then he would have to prove his efforts to find new tenants. But that is going to include court.Answer #2: not legal advice:\\n\\nI would call your landlord up, and tell him you noticed he was renovating the place, and because of that he's not meeting the requirements of the law.  so, rather than sue him to get your 1400 back, you're willing to drop the matter in exchange for not paying the remaining month of the lease.  \\n\\nrecord the conversation</td>\n",
       "      <td>04-22-2015</td>\n",
       "      <td>11-11-2021</td>\n",
       "      <td>http://www.reddit.com/r/legaladvice/comments/33icba/ustx_renters_rights_moved_out_three_months_before/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Title: I live in Cyprus. I own a convenience store and the mayor gave me a 21 day notice to demolish the pergola outside where I keep my newspapers, magazines and other stock items.\\nQuestion:I own a convenience store and the mayor gave me a 21 day notice to demolish the pergola outside where I keep my newspapers, magazines and other stock items. The mayor gave verbal instructions on how he wants all convenience stores' pergolas to be but he had not put anything down on papers. I live in Cyprus. Is there anything I can do in order to earn time to get the amount of money needed to make the modification needed to be legal. \\nAnswer #1: Not an expert on Cyprus by any means, but whether the pergola is legal is probably a matter of local ordinance or zoning.  Is there an easy way to access your local ordinances?  If the pergola is legal, there should be administrative procedures in order to object/dispute the notice (maybe the notice tells you what you can do if you disagree).\\n\\nThat said, you can always call and ask for more time and see if they agree.</td>\n",
       "      <td>11-12-2015</td>\n",
       "      <td>11-10-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/3sk703/i_live_in_cyprus_i_own_a_convenience_store_and/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Title: Apartment air conditioning not cooling properly, can I withhold rent?\\nQuestion:I live in a huge apartment complex and I live on the third floor, which is the top floor.  I live in Missouri so it's usually around 90 degrees outside and when I get back from work around 5pm, my apartment is usually 80 degrees.  I set my AC around 68-70 and by 10 PM it is only 76 degrees, 74 if I'm lucky.  Maybe I've just had good AC systems my entire life but back at home it will hit the desired temperature within an hour.  \\n\\nI have complained to the main office and they said this is normal for AC systems, especially since I am on the top floor.  They said if I want it to be 75 degrees when I get back to my apartment, I should leave my AC on while at work, which I absolutely will not do since it will probably double or triple my current electric bill.  My electric bill is also relatively high considering I live in a one bedroom apartment and have to keep my AC running more than I would like to since it's so hot in my apartment.  The AC that I am used to will run for about 30-60 minutes before it reaches the set temperature and will turn on every so often to maintain the temperature.  I have put in several maintenance requests but I do not think the maintenance guy actually does anything.  \\n\\nWhat can I do?  Can I ask them to pay half of my electric bill?  Can I withhold rent?  I just want to be comfortable in my own apartment.  Please let me know if there's anything else you would like to know about my situation.\\nAnswer #1: Your apartment complex is right. If you let your apartment heat up all day the AC is going to have a very hard time cooling it off before you head to bed. \\n\\nIt cooling down to 74 is likely acceptable and there isn't much you can do beyond either trying to run it whil eyou are at work or getting a window unit.</td>\n",
       "      <td>07-10-2018</td>\n",
       "      <td>09-29-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/8xt1ph/apartment_air_conditioning_not_cooling_properly/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Title: Is it appropriate to ask HR the specifics of a pre employment drug test.\\nQuestion:Iowa. Been offered a job contingent on a drug test.  Is it ok to ask HR for the number of panels on the test. 5,8 or 10. \\nAnswer #1: You can, and that will immediately trigger warning signs with themAnswer #2: Sure, you can ask if you want to.  Nothing stopping you.\\n\\nThat will likely disqualify you from further consideration, and you'd have no recourse for that.Answer #3: You can, and they're likely to DQ you. Unless you're on something like prescription pain meds, the question will immediately raise red flags.</td>\n",
       "      <td>05-30-2017</td>\n",
       "      <td>10-02-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/6e7dmc/is_it_appropriate_to_ask_hr_the_specifics_of_a/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Title: An insane dog owner thinks my dad stole one of his puppies.\\nQuestion:Ok I'm sorry for formatting or anything I'm on mobile. \\n\\nOk so some back story: I work at a restaurant in Minnesota, my lets just call him Steve (S), and there is a (mentally unstable) homeless person who used to come in and buy food all the time. We will call him Jay (J), so Jay is homeless and says he is starting a service dog company, out of the back of his van. He has been keeping about seven dogs in the back and a lot of them seem malnourished.\\n\\nSo one day, my dad (D), S, and J are all outside the restaurant, playing with the dogs. My boss tells me all about J and how he isn't mentally healthy. He then says that he was talking to him and he is going to get the dogs in his custody so he can give them to people who can take care of them. So he gets about 3 of the dogs from J and says he will help train them. Keep in mind J has given the dogs to Steve and helped the dogs into Steves car, with witnesses, mainly me and my dad. So Steve kept the dogs and J called the cops to report it.\\n\\nAfter the report S explained everything and said me and D were witnesses. The cop said Ok everything seems made up and we thought that was the end of it.\\n\\nToday, D gets a letter from J's (supposed) attorney. It says that we took a dog, and have to give it back or pay the amount of $20,000. We were just there as witnesses and never even brought a dog to our house. This seems like a scare tactic, but we have no idea for sure. What can we do in this situation, is there any way to tell if this is just a scare tactic or not, (how did he even get our address), and should we call the number they provided us or should we just ignore it? Please answer the three questions not in parentheses and I'll work on the one in it. Thanks for any advice.\\nAnswer #1: No real attorney is going to represent a homeless guy asking for $20,000 for a puppy. \\n\\nIgnore unless you are served with a notice from the court. \\n\\nCall the ASPCA or animal control to report J for animal abuse. \\n\\n</td>\n",
       "      <td>10-18-2018</td>\n",
       "      <td>09-29-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/9pfsik/an_insane_dog_owner_thinks_my_dad_stole_one_of/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Title: My Mother took student loans out in my name, but I never saw any of the money.\\nQuestion:In 2016, I attended college for one semester. My Father and Mother are separated, but had an education fund for me in place, so I never needed to take out money with OSAP (Ontario student loans). I have a permanent disability, so on my behalf with my permission, my Mother applied for a grant pertaining to Students with Permanent Disabilities (approx $700). This was the only grant I knew of.\\n\\nLast evening, I called CRA to get my tax return information sorted. The agent informed me that OSAP took my tax return (approx $1500), and I was terribly confused as I never took out a loan with OSAP. After answering security questions, I signed onto OSAP and found that another Grant for First Year Students (approx $700) was also released to me. I didn’t see that money. Another grant from Middle Class income Families was released (approx $2500) but again, that is money I didn’t see. Another amount for approx $400 was released, though I can’t remember what for.\\n\\nI understand to an extent that should I drop out before 30 (?) days, grants become loans. But, I wasn’t aware that anything other than $700 was released to me. I never saw the other money, or what was done with it, and again,  I had an education fund so I didn’t think I owed OSAP.\\n\\nI was 19 when I went to college, so I did give my Mother permission to apply for one grant, but not the others. Again, I never saw that money. All the contact info lead to her, and I never knew even if they needed money from me, because the mailing info was hers.\\n\\nWhat do I do here? She’s highly uncooperative and we aren’t on good terms. I needed that $1500 as I have a child on the way, but now I’m owing and I feel so distraught. \\n\\nThanks\\nTopic:\\nCanada\\nAnswer #1: Giving your mother permission to apply for one grant doesn't give her permission to apply for multiple ones. That's fraud on her part. If you so choose, you can ask your mother to pay back the amount of the loans. Or you can report the identity theft to the police and then they will investigate. Then contact the loan holder and tell them that someone else took out the loans and you are a victim of identity theft. They'll let you know what information you need to provide. \\n\\nWhile I hesitate to say this, before making any decisions, please consider the legal and social ramifications to your family (mother). With that being said, you are 100% legally justified in reporting her to the police.</td>\n",
       "      <td>04-13-2019</td>\n",
       "      <td>09-27-2021</td>\n",
       "      <td>https://www.reddit.com/r/legaladvice/comments/bcr81v/my_mother_took_student_loans_out_in_my_name_but_i/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(dataset[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b009d700-0109-4cb7-a2ac-4c3acc523073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c558a56f5a442738ce4e6bb85e07af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784e87dfd1cc4cc38e5bee4467f17e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/opt-125m were not used when initializing OPTForCausalLM: ['model.decoder.final_layer_norm.weight', 'model.decoder.final_layer_norm.bias']\n",
      "- This IS expected if you are initializing OPTForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OPTForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eba3d0f-e9fe-4923-bdc0-e382271523fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17064e5a-f5f2-4897-aec4-3737ffb00ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Title: Personal information and walmart\\nQuestion:This is whats going on. I bought a nerf gun for my son at Wal-Mart for Christmas. I found the same nerf gun $40 cheaper, so naturally returned it to Wal-Mart. I got a call yesterday (well voicemail) from a Wal-Mart customer who bought the nerf gun.. it gets weird here.. they said Wal-Mart put my personal information on the box and decided to use it to contact me to see if there is anything wrong with it. I am naturally paranoid with my personal information as is most people. Why would they do this? What should I do? sorry if I am in the wrong sub-reddit.. Thanks in advance \\n\\np.s. I do not frequent Wal-Mart often, it was the only place for this particular nerf gun at the moment \\n\\nI am in the chicago area \\nAnswer #1: Your information is personal in nature, but is by no means private.  Your full name, birth date, home address, and sometimes phone numbers are all generally a matter of public record and readily available to anyone who wants them.\\n\\nIt may be weird, but I don't believe it to be illegal.  You can choose to answer or not answer their questions, and you can tell them to not contact you about the Nerf gun in the future.\",\n",
       " 'created_timestamp': '01-10-2018',\n",
       " 'downloaded_timestamp': '10-01-2021',\n",
       " 'url': 'https://www.reddit.com/r/legaladvice/comments/7phbeu/personal_information_and_walmart/'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][161]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf497114-5bb5-4bb3-8938-03eaca375bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 46525, 35, 13129, 335, 8, 21788, 15964, 50118, 45641, 35, 713, 16, 45676, 164, 15, 4, 38, 2162, 10, 38286, 506, 1751, 13, 127, 979, 23, 6092, 12, 13012, 13, 1619, 4, 38, 303, 5, 276, 38286, 506, 1751, 68, 1749, 7246, 6, 98, 8366, 1835, 24, 7, 6092, 12, 13012, 4, 38, 300, 10, 486, 2350, 36, 3056, 30118, 10555, 43, 31, 10, 6092, 12, 13012, 2111, 54, 2162, 5, 38286, 506, 1751, 7586, 24, 1516, 7735, 259, 7586, 51, 26, 6092, 12, 13012, 342, 127, 1081, 335, 15, 5, 2233, 8, 1276, 7, 304, 24, 7, 1511, 162, 7, 192, 114, 89, 16, 932, 1593, 19, 24, 4, 38, 524, 8366, 33554, 19, 127, 1081, 335, 25, 16, 144, 82, 4, 2612, 74, 51, 109, 42, 116, 653, 197, 38, 109, 116, 6661, 114, 38, 524, 11, 5, 1593, 2849, 12, 48724, 7586, 4557, 11, 3316, 1437, 50118, 50118, 642, 4, 29, 4, 38, 109, 45, 7690, 6092, 12, 13012, 747, 6, 24, 21, 5, 129, 317, 13, 42, 1989, 38286, 506, 1751, 23, 5, 1151, 1437, 50118, 50118, 100, 524, 11, 5, 1855, 42938, 443, 1437, 50118, 33683, 849, 134, 35, 2486, 335, 16, 1081, 11, 2574, 6, 53, 16, 30, 117, 839, 940, 4, 1437, 2486, 455, 766, 6, 3113, 1248, 6, 184, 1100, 6, 8, 2128, 1028, 1530, 32, 70, 3489, 10, 948, 9, 285, 638, 8, 16650, 577, 7, 1268, 54, 1072, 106, 4, 50118, 50118, 243, 189, 28, 7735, 6, 53, 38, 218, 75, 679, 24, 7, 28, 2439, 4, 1437, 370, 64, 2807, 7, 1948, 50, 45, 1948, 49, 1142, 6, 8, 47, 64, 1137, 106, 7, 45, 1511, 47, 59, 5, 24579, 506, 1751, 11, 5, 499, 4], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function(dataset[\"train\"][161])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604e9e1c-088a-47b1-a044-6286618a636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e384f17302c42e5a5b9c58cd24b923c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562d4503695a41c9a3a34bfb2fcd5449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db38625e37b48ae8bf942dd966c5721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003ba41f410a4b7bbe06b697cc712742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3321749587d44a14baba416a16eb6a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff883c19cf34424b1e737a5de033211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672460f63c6f4671bb5e9996ba249e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a766d63fc0784e959fa92bcf29746e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e95fc11b1c5c45a2835b01a9ad6aee40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd26502e0794523a3ff7b6d58142d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8785114486254fff879d2835a89b33b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c4eb295340455484a296c44a01b85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96cbe3263c604b3294215f95b3e97717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7edb72da600a45c5a63a39e3b26bbff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b35a4d6cf544b0b348b591ec76a1d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd23e424a66488bbd0b9d718e35c0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=8, remove_columns=[\"text\",\"created_timestamp\",\"downloaded_timestamp\",\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2789efa7-61b6-45c1-a6a9-51dbe2ced65f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2,\n",
       "  46525,\n",
       "  35,\n",
       "  38,\n",
       "  206,\n",
       "  951,\n",
       "  16,\n",
       "  3433,\n",
       "  88,\n",
       "  127,\n",
       "  512,\n",
       "  7,\n",
       "  120,\n",
       "  13543,\n",
       "  13,\n",
       "  162,\n",
       "  145,\n",
       "  10,\n",
       "  1099,\n",
       "  1393,\n",
       "  411,\n",
       "  377,\n",
       "  536,\n",
       "  50118,\n",
       "  45641,\n",
       "  35,\n",
       "  713,\n",
       "  16,\n",
       "  11,\n",
       "  2042,\n",
       "  4,\n",
       "  3727,\n",
       "  11,\n",
       "  392,\n",
       "  6,\n",
       "  38,\n",
       "  13636,\n",
       "  4024,\n",
       "  5,\n",
       "  1593,\n",
       "  169,\n",
       "  159,\n",
       "  10,\n",
       "  5342,\n",
       "  3610,\n",
       "  65,\n",
       "  169,\n",
       "  921,\n",
       "  142,\n",
       "  38,\n",
       "  56,\n",
       "  45,\n",
       "  551,\n",
       "  127,\n",
       "  33286,\n",
       "  5679,\n",
       "  29,\n",
       "  14,\n",
       "  183,\n",
       "  4,\n",
       "  17397,\n",
       "  117,\n",
       "  1677,\n",
       "  58,\n",
       "  567,\n",
       "  1567,\n",
       "  162,\n",
       "  8,\n",
       "  25,\n",
       "  1010,\n",
       "  25,\n",
       "  38,\n",
       "  5426,\n",
       "  24,\n",
       "  6,\n",
       "  38,\n",
       "  4024,\n",
       "  11,\n",
       "  5,\n",
       "  4806,\n",
       "  6625,\n",
       "  13,\n",
       "  5,\n",
       "  1079,\n",
       "  9,\n",
       "  5,\n",
       "  1803,\n",
       "  454,\n",
       "  38,\n",
       "  115,\n",
       "  1004,\n",
       "  4,\n",
       "  38,\n",
       "  21,\n",
       "  2198,\n",
       "  7619,\n",
       "  8435,\n",
       "  66,\n",
       "  59,\n",
       "  24,\n",
       "  6,\n",
       "  98,\n",
       "  38,\n",
       "  1276,\n",
       "  7,\n",
       "  213,\n",
       "  124,\n",
       "  184,\n",
       "  8,\n",
       "  38003,\n",
       "  2185,\n",
       "  4,\n",
       "  38,\n",
       "  1299,\n",
       "  3668,\n",
       "  6587,\n",
       "  59,\n",
       "  24,\n",
       "  4,\n",
       "  287,\n",
       "  38,\n",
       "  21,\n",
       "  1428,\n",
       "  184,\n",
       "  6,\n",
       "  38,\n",
       "  5324,\n",
       "  10,\n",
       "  512,\n",
       "  511,\n",
       "  162,\n",
       "  4,\n",
       "  38,\n",
       "  554,\n",
       "  7,\n",
       "  185,\n",
       "  124,\n",
       "  3197,\n",
       "  8,\n",
       "  1076,\n",
       "  21242,\n",
       "  7,\n",
       "  860,\n",
       "  7,\n",
       "  2217,\n",
       "  123,\n",
       "  13,\n",
       "  59,\n",
       "  379,\n",
       "  728,\n",
       "  6,\n",
       "  53,\n",
       "  38,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  4,\n",
       "  152,\n",
       "  7619,\n",
       "  8435,\n",
       "  162,\n",
       "  66,\n",
       "  190,\n",
       "  55,\n",
       "  8,\n",
       "  38,\n",
       "  1299,\n",
       "  101,\n",
       "  38,\n",
       "  21,\n",
       "  350,\n",
       "  16573,\n",
       "  7,\n",
       "  1305,\n",
       "  6,\n",
       "  98,\n",
       "  38,\n",
       "  1276,\n",
       "  7,\n",
       "  4757,\n",
       "  19,\n",
       "  5,\n",
       "  563,\n",
       "  8,\n",
       "  213,\n",
       "  184,\n",
       "  4,\n",
       "  287,\n",
       "  1010,\n",
       "  25,\n",
       "  38,\n",
       "  300,\n",
       "  66,\n",
       "  9,\n",
       "  127,\n",
       "  512,\n",
       "  6,\n",
       "  37,\n",
       "  2468,\n",
       "  62,\n",
       "  220,\n",
       "  7,\n",
       "  162,\n",
       "  6,\n",
       "  6387,\n",
       "  159,\n",
       "  39,\n",
       "  2931,\n",
       "  6,\n",
       "  8,\n",
       "  554,\n",
       "  11347,\n",
       "  23,\n",
       "  162,\n",
       "  4,\n",
       "  91,\n",
       "  21,\n",
       "  740,\n",
       "  4781,\n",
       "  154,\n",
       "  162,\n",
       "  66,\n",
       "  6,\n",
       "  2758,\n",
       "  162,\n",
       "  7,\n",
       "  213,\n",
       "  3549,\n",
       "  2185,\n",
       "  6,\n",
       "  8,\n",
       "  5608,\n",
       "  162,\n",
       "  4,\n",
       "  85,\n",
       "  21,\n",
       "  169,\n",
       "  375,\n",
       "  2340,\n",
       "  921,\n",
       "  14706,\n",
       "  4,\n",
       "  38,\n",
       "  399,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  224,\n",
       "  932,\n",
       "  8,\n",
       "  95,\n",
       "  3203,\n",
       "  88,\n",
       "  127,\n",
       "  2632,\n",
       "  4,\n",
       "  6068,\n",
       "  14,\n",
       "  37,\n",
       "  115,\n",
       "  45,\n",
       "  33,\n",
       "  684,\n",
       "  61,\n",
       "  3537,\n",
       "  38,\n",
       "  21,\n",
       "  11,\n",
       "  6,\n",
       "  3680,\n",
       "  38,\n",
       "  1979,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  33,\n",
       "  1613,\n",
       "  1025,\n",
       "  4,\n",
       "  38,\n",
       "  4711,\n",
       "  89,\n",
       "  13,\n",
       "  59,\n",
       "  389,\n",
       "  12,\n",
       "  1898,\n",
       "  728,\n",
       "  4,\n",
       "  38,\n",
       "  376,\n",
       "  124,\n",
       "  66,\n",
       "  8,\n",
       "  300,\n",
       "  11,\n",
       "  127,\n",
       "  512,\n",
       "  456,\n",
       "  6,\n",
       "  172,\n",
       "  554,\n",
       "  1428,\n",
       "  4,\n",
       "  38,\n",
       "  794,\n",
       "  14,\n",
       "  37,\n",
       "  56,\n",
       "  2294,\n",
       "  11,\n",
       "  41,\n",
       "  18743,\n",
       "  8,\n",
       "  56,\n",
       "  9010,\n",
       "  13,\n",
       "  162,\n",
       "  4,\n",
       "  91,\n",
       "  880,\n",
       "  7,\n",
       "  1407,\n",
       "  162,\n",
       "  456,\n",
       "  4,\n",
       "  38,\n",
       "  21,\n",
       "  441,\n",
       "  7,\n",
       "  2217,\n",
       "  123,\n",
       "  8,\n",
       "  7433,\n",
       "  127,\n",
       "  512,\n",
       "  639,\n",
       "  10,\n",
       "  745,\n",
       "  147,\n",
       "  37,\n",
       "  1705,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  465,\n",
       "  162,\n",
       "  4,\n",
       "  1437,\n",
       "  50118,\n",
       "  50118,\n",
       "  35515,\n",
       "  556,\n",
       "  7,\n",
       "  5,\n",
       "  253,\n",
       "  9,\n",
       "  550,\n",
       "  4,\n",
       "  38,\n",
       "  21,\n",
       "  602,\n",
       "  10,\n",
       "  1805,\n",
       "  66,\n",
       "  9,\n",
       "  5,\n",
       "  247,\n",
       "  8,\n",
       "  5,\n",
       "  363,\n",
       "  137,\n",
       "  38,\n",
       "  314,\n",
       "  38,\n",
       "  362,\n",
       "  65,\n",
       "  9,\n",
       "  127,\n",
       "  3235,\n",
       "  28162,\n",
       "  66,\n",
       "  7,\n",
       "  127,\n",
       "  512,\n",
       "  4,\n",
       "  38,\n",
       "  524,\n",
       "  29206,\n",
       "  2068,\n",
       "  10002,\n",
       "  38,\n",
       "  5930,\n",
       "  127,\n",
       "  1883,\n",
       "  4,\n",
       "  520,\n",
       "  38,\n",
       "  439,\n",
       "  66,\n",
       "  419,\n",
       "  5,\n",
       "  220,\n",
       "  662,\n",
       "  6,\n",
       "  5,\n",
       "  1393,\n",
       "  1883,\n",
       "  21,\n",
       "  1810,\n",
       "  490,\n",
       "  19,\n",
       "  5,\n",
       "  512,\n",
       "  2664,\n",
       "  9512,\n",
       "  4,\n",
       "  20,\n",
       "  28441,\n",
       "  6,\n",
       "  19015,\n",
       "  2233,\n",
       "  6,\n",
       "  8,\n",
       "  12304,\n",
       "  58,\n",
       "  12961,\n",
       "  6128,\n",
       "  6,\n",
       "  8,\n",
       "  5,\n",
       "  14016,\n",
       "  4328,\n",
       "  857,\n",
       "  21,\n",
       "  12256,\n",
       "  4102,\n",
       "  4,\n",
       "  85,\n",
       "  21,\n",
       "  1099,\n",
       "  615,\n",
       "  14,\n",
       "  24,\n",
       "  2551,\n",
       "  101,\n",
       "  51,\n",
       "  56,\n",
       "  1240,\n",
       "  1823,\n",
       "  86,\n",
       "  442,\n",
       "  686,\n",
       "  5,\n",
       "  512,\n",
       "  21,\n",
       "  10,\n",
       "  4463,\n",
       "  4,\n",
       "  345,\n",
       "  21,\n",
       "  1085,\n",
       "  5130,\n",
       "  8,\n",
       "  70,\n",
       "  14,\n",
       "  21,\n",
       "  551,\n",
       "  21,\n",
       "  127,\n",
       "  17637,\n",
       "  510,\n",
       "  20529,\n",
       "  6,\n",
       "  103,\n",
       "  14967,\n",
       "  6,\n",
       "  10,\n",
       "  6036,\n",
       "  29,\n",
       "  4085,\n",
       "  1886,\n",
       "  6,\n",
       "  8,\n",
       "  10,\n",
       "  2003,\n",
       "  9,\n",
       "  13938,\n",
       "  4,\n",
       "  1308,\n",
       "  68,\n",
       "  2619,\n",
       "  1690,\n",
       "  4774,\n",
       "  31468,\n",
       "  21,\n",
       "  314,\n",
       "  639,\n",
       "  4,\n",
       "  252,\n",
       "  314,\n",
       "  10,\n",
       "  6356,\n",
       "  9,\n",
       "  14407,\n",
       "  8,\n",
       "  10,\n",
       "  13026,\n",
       "  15,\n",
       "  5,\n",
       "  2377,\n",
       "  2418,\n",
       "  4,\n",
       "  440,\n",
       "  97,\n",
       "  1677,\n",
       "  58,\n",
       "  3187,\n",
       "  88,\n",
       "  4,\n",
       "  38,\n",
       "  222,\n",
       "  486,\n",
       "  5,\n",
       "  249,\n",
       "  6,\n",
       "  8,\n",
       "  9010,\n",
       "  41,\n",
       "  1946,\n",
       "  13,\n",
       "  106,\n",
       "  6,\n",
       "  53,\n",
       "  38,\n",
       "  956,\n",
       "  7,\n",
       "  989,\n",
       "  98,\n",
       "  38,\n",
       "  56,\n",
       "  7,\n",
       "  10061,\n",
       "  5,\n",
       "  486,\n",
       "  4,\n",
       "  598,\n",
       "  162,\n",
       "  6,\n",
       "  89,\n",
       "  21,\n",
       "  1085,\n",
       "  2422,\n",
       "  7775,\n",
       "  59,\n",
       "  24,\n",
       "  4,\n",
       "  85,\n",
       "  156,\n",
       "  1472,\n",
       "  1782,\n",
       "  25,\n",
       "  89,\n",
       "  21,\n",
       "  10,\n",
       "  28441,\n",
       "  11,\n",
       "  5,\n",
       "  124,\n",
       "  11517,\n",
       "  4,\n",
       "  635,\n",
       "  6,\n",
       "  127,\n",
       "  512,\n",
       "  21,\n",
       "  5,\n",
       "  129,\n",
       "  65,\n",
       "  11,\n",
       "  5,\n",
       "  319,\n",
       "  14,\n",
       "  115,\n",
       "  28,\n",
       "  450,\n",
       "  31,\n",
       "  5,\n",
       "  921,\n",
       "  6,\n",
       "  98,\n",
       "  24,\n",
       "  21,\n",
       "  10,\n",
       "  828,\n",
       "  7782,\n",
       "  14,\n",
       "  51,\n",
       "  4689,\n",
       "  127,\n",
       "  512,\n",
       "  4,\n",
       "  6811,\n",
       "  38,\n",
       "  2967,\n",
       "  14,\n",
       "  127,\n",
       "  2632,\n",
       "  473,\n",
       "  33,\n",
       "  573,\n",
       "  4387,\n",
       "  6,\n",
       "  53,\n",
       "  30,\n",
       "  172,\n",
       "  24,\n",
       "  21,\n",
       "  350,\n",
       "  628,\n",
       "  8,\n",
       "  51,\n",
       "  117,\n",
       "  1181,\n",
       "  56,\n",
       "  5,\n",
       "  4338,\n",
       "  4,\n",
       "  1308,\n",
       "  19812,\n",
       "  174,\n",
       "  162,\n",
       "  14,\n",
       "  117,\n",
       "  512,\n",
       "  34,\n",
       "  655,\n",
       "  57,\n",
       "  3187,\n",
       "  88,\n",
       "  11,\n",
       "  14,\n",
       "  319,\n",
       "  4,\n",
       "  38,\n",
       "  21,\n",
       "  1613,\n",
       "  13,\n",
       "  10,\n",
       "  353,\n",
       "  8,\n",
       "  127,\n",
       "  512,\n",
       "  21,\n",
       "  45,\n",
       "  89,\n",
       "  148,\n",
       "  14,\n",
       "  86,\n",
       "  4,\n",
       "  1437,\n",
       "  50118,\n",
       "  50118,\n",
       "  35515,\n",
       "  556,\n",
       "  7,\n",
       "  452,\n",
       "  4,\n",
       "  38,\n",
       "  439,\n",
       "  66,\n",
       "  42,\n",
       "  662,\n",
       "  8,\n",
       "  5426,\n",
       "  127,\n",
       "  512,\n",
       "  56,\n",
       "  57,\n",
       "  3187,\n",
       "  88,\n",
       "  456,\n",
       "  4,\n",
       "  7574,\n",
       "  6,\n",
       "  38,\n",
       "  56,\n",
       "  5930,\n",
       "  24,\n",
       "  6,\n",
       "  53,\n",
       "  42,\n",
       "  86,\n",
       "  5,\n",
       "  4408,\n",
       "  1883,\n",
       "  21,\n",
       "  1759,\n",
       "  5686,\n",
       "  490,\n",
       "  4,\n",
       "  345,\n",
       "  21,\n",
       "  3668,\n",
       "  1085,\n",
       "  11,\n",
       "  5,\n",
       "  512,\n",
       "  14,\n",
       "  74,\n",
       "  492,\n",
       "  951,\n",
       "  10,\n",
       "  1219,\n",
       "  7,\n",
       "  1108,\n",
       "  11,\n",
       "  4,\n",
       "  20,\n",
       "  1312,\n",
       "  12304,\n",
       "  8,\n",
       "  19015,\n",
       "  2233,\n",
       "  58,\n",
       "  314,\n",
       "  490,\n",
       "  4,\n",
       "  20,\n",
       "  14407,\n",
       "  314,\n",
       "  639,\n",
       "  94,\n",
       "  86,\n",
       "  58,\n",
       "  202,\n",
       "  11,\n",
       "  5,\n",
       "  12304,\n",
       "  4,\n",
       "  10385,\n",
       "  21,\n",
       "  551,\n",
       "  8,\n",
       "  117,\n",
       "  97,\n",
       "  1677,\n",
       "  58,\n",
       "  3187,\n",
       "  88,\n",
       "  4,\n",
       "  653,\n",
       "  21,\n",
       "  7775,\n",
       "  7,\n",
       "  162,\n",
       "  21,\n",
       "  14,\n",
       "  114,\n",
       "  51,\n",
       "  74,\n",
       "  33,\n",
       "  1367,\n",
       "  5,\n",
       "  12304,\n",
       "  6,\n",
       "  19015,\n",
       "  2233,\n",
       "  6,\n",
       "  8,\n",
       "  1883,\n",
       "  6,\n",
       "  38,\n",
       "  74,\n",
       "  33,\n",
       "  56,\n",
       "  117,\n",
       "  1114,\n",
       "  24,\n",
       "  21,\n",
       "  3187,\n",
       "  88,\n",
       "  4,\n",
       "  1437,\n",
       "  50118,\n",
       "  50118,\n",
       "  100,\n",
       "  33,\n",
       "  20208,\n",
       "  127,\n",
       "  2900,\n",
       "  8,\n",
       "  38,\n",
       "  1395,\n",
       "  206,\n",
       "  9,\n",
       "  1268,\n",
       "  1493,\n",
       "  14,\n",
       "  115,\n",
       "  28,\n",
       "  608,\n",
       "  42,\n",
       "  8,\n",
       "  24,\n",
       "  630,\n",
       "  75,\n",
       "  2045,\n",
       "  101,\n",
       "  10,\n",
       "  21279,\n",
       "  14,\n",
       "  42,\n",
       "  1102,\n",
       "  2330,\n",
       "  11,\n",
       "  2230,\n",
       "  5,\n",
       "  276,\n",
       "  169,\n",
       "  4,\n",
       "  29175,\n",
       "  14,\n",
       "  951,\n",
       "  16,\n",
       "  888,\n",
       "  42,\n",
       "  5373,\n",
       "  6,\n",
       "  16,\n",
       "  89,\n",
       "  143,\n",
       "  1030,\n",
       "  814,\n",
       "  14,\n",
       "  38,\n",
       "  64,\n",
       "  185,\n",
       "  116,\n",
       "  345,\n",
       "  16,\n",
       "  9261,\n",
       "  1493,\n",
       "  14,\n",
       "  38,\n",
       "  64,\n",
       "  2221,\n",
       "  127,\n",
       "  512,\n",
       "  4,\n",
       "  318,\n",
       "  38,\n",
       "  192,\n",
       "  5,\n",
       "  573,\n",
       "  4338,\n",
       "  8,\n",
       "  38,\n",
       "  524,\n",
       "  441,\n",
       "  7,\n",
       "  3058,\n",
       "  114,\n",
       "  24,\n",
       "  16,\n",
       "  123,\n",
       "  6,\n",
       "  99,\n",
       "  64,\n",
       "  38,\n",
       "  109,\n",
       "  7,\n",
       "  912,\n",
       "  42,\n",
       "  31,\n",
       "  2909,\n",
       "  456,\n",
       "  116,\n",
       "  404,\n",
       "  38,\n",
       "  216,\n",
       "  16,\n",
       "  99,\n",
       "  37,\n",
       "  1326,\n",
       "  101,\n",
       "  36,\n",
       "  700,\n",
       "  56,\n",
       "  11693,\n",
       "  21462,\n",
       "  43,\n",
       "  8,\n",
       "  14,\n",
       "  37,\n",
       "  6790,\n",
       "  10,\n",
       "  4334,\n",
       "  15283,\n",
       "  4,\n",
       "  38,\n",
       "  218,\n",
       "  75,\n",
       "  216,\n",
       "  39,\n",
       "  766,\n",
       "  8,\n",
       "  25,\n",
       "  444,\n",
       "  25,\n",
       "  38,\n",
       "  216,\n",
       "  37,\n",
       "  473,\n",
       "  45,\n",
       "  216,\n",
       "  99,\n",
       "  127,\n",
       "  766,\n",
       "  16,\n",
       "  50,\n",
       "  127,\n",
       "  3537,\n",
       "  346,\n",
       "  4,\n",
       "  1437,\n",
       "  50118,\n",
       "  33683,\n",
       "  849,\n",
       "  134,\n",
       "  35,\n",
       "  8655,\n",
       "  10,\n",
       "  249,\n",
       "  266,\n",
       "  13,\n",
       "  358,\n",
       "  1108,\n",
       "  11,\n",
       "  8,\n",
       "  120,\n",
       "  10,\n",
       "  12575,\n",
       "  11021,\n",
       "  4,\n",
       "  38,\n",
       "  206,\n",
       "  47,\n",
       "  17,\n",
       "  27,\n",
       "  241,\n",
       "  2542,\n",
       "  14,\n",
       "  5,\n",
       "  249,\n",
       "  351,\n",
       "  17,\n",
       "  27,\n",
       "  90,\n",
       "  533,\n",
       "  28,\n",
       "  441,\n",
       "  7,\n",
       "  3094,\n",
       "  54,\n",
       "  5,\n",
       "  14542,\n",
       "  16,\n",
       "  31,\n",
       "  10,\n",
       "  1345,\n",
       "  8,\n",
       "  8194,\n",
       "  9,\n",
       "  5,\n",
       "  512,\n",
       "  103,\n",
       "  801,\n",
       "  1984,\n",
       "  4024,\n",
       "  14,\n",
       "  183,\n",
       "  4,\n",
       "  497,\n",
       "  275,\n",
       "  6,\n",
       "  114,\n",
       "  47,\n",
       "  386,\n",
       "  122,\n",
       "  8,\n",
       "  42,\n",
       "  1388,\n",
       "  2909,\n",
       "  6,\n",
       "  2085,\n",
       "  47,\n",
       "  64,\n",
       "  33172,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958bb934-3644-4555-ab82-b77163b1a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block_size = tokenizer.model_max_length\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0fbea33-b914-47c0-8052-19761da86565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51992ed7-ee09-4add-b5ce-3cb5fe1a9a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702052b60ad74c9fb277dda9c1041a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43035780f11241edacb2c87c181711ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a17a4c1a3194024bd5578bc04eba762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e876da74314f9e8b5da5956faaa483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "278afeb4a36345e08d121d56cb1d3b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22d18ae54ff4b5d9216e4839a898c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98402ca8da7c4a5fbec3a7c318273275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f609a0cd22c4cb693a07c9b1c428c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae203d4dcec4cdb901c4cf55415f792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d90db22ffad546d5aeacd23ba4b66f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58ae8e833ecc4d6e83be242bdc9862d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e3f9e8681d46cd97e290adefd7bf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#7:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b2cd9d749741fb8457ae4ac8c4f6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#6:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896aca3db81946399709b5425ebaa67f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d834d648be4371b78d53e6c9fe5f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#5:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b499637f6a41ecb929d00c14404af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#4:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_datasets = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48573f64-e23d-41a8-a342-12144e6d6bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cpu_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        f\"opt-test\",\n",
    "\n",
    "    no_cuda= True,\n",
    "    bf16=True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    eval_dataset=lm_datasets[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96d5df51-0a70-4630-b7ea-70c81575d97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 405492\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 152061\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='152061' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     2/152061 : < :, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:1371\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1368\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1370\u001b[0m )\n\u001b[0;32m-> 1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:1613\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1611\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1613\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1616\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1619\u001b[0m ):\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1621\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:2307\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2306\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2307\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2310\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/trainer.py:2339\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2338\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2339\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2341\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:908\u001b[0m, in \u001b[0;36mOPTForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    905\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    922\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:674\u001b[0m, in \u001b[0;36mOPTDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    665\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    666\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    667\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    671\u001b[0m     )\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 674\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:339\u001b[0m, in \u001b[0;36mOPTDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    336\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states)\n\u001b[1;32m    337\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(hidden_states)\n\u001b[0;32m--> 339\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    342\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m (residual \u001b[38;5;241m+\u001b[39m hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_states_shape)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/hf/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31da10c7-700f-4cf3-b33f-647a6a9dc19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05d4ada-7d57-4767-860f-d64cc2e5b6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_num_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18c147-444d-4d9a-af1a-97f9be5ba23a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
